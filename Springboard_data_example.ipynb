{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Springboard data collection function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this function is to take the raw hourly footfall data which is provided via the Springboard API, and then convert it into a simplified daily footfall table in the format:\n",
    "\n",
    "| Date | Camera 1 | Camera 2 | Camera 3 | etc. |\n",
    "|-|-|-|--|\n",
    "| 20/07/2017 | 536 | 891 | 362 | etc. |\n",
    "| 21/07/2017 | 478| 1002| 254 | etc. |\n",
    "| 22/07/2017 | 625 | 1025 | 362 | etc. |\n",
    "\n",
    "The output will be saved as a CSV at the filepath supplied as the `new_data_filepath` argument. If this argument references an existing CSV file that already contains data in the above format, then the function will automatically find any new data available via the API from since the last data was collected.\n",
    "\n",
    "The `primary_camera_name` argument is required in order that the function can find the date when data was last collected - it does not matter which camera name is used, as long as the camera supplied was active the last time the function was used to collect data.\n",
    "\n",
    "If the optional argument `backup_data_filepath` is supplied then any existing data will be written to that location before it is over-written with new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Custom error\n",
    "class SpringboardError(Exception):\n",
    "    pass\n",
    "\n",
    "def Springboard_data(useremail, userpassword,\n",
    "                     new_data_filepath,\n",
    "                     primary_camera_name,\n",
    "                     backup_data_filepath = None):\n",
    "    \n",
    "    # Read in the old data if it exists, else create new df.\n",
    "    try:\n",
    "        ff_to_date = pd.read_csv(new_data_filepath, index_col = 0)\n",
    "        \n",
    "        # Change index to proper DateTime\n",
    "        ff_to_date.index = pd.to_datetime(ff_to_date.index, dayfirst = True)\n",
    "        \n",
    "        # Latest date with data\n",
    "        latest_date = max(ff_to_date[ff_to_date[primary_camera_name].notnull()].index)\n",
    "        \n",
    "        # Day after most recent data as the start date for the API request.\n",
    "        startdate = (latest_date + datetime.timedelta(1)).strftime(\"%Y%m%d\")\n",
    "        \n",
    "         # First date with data\n",
    "        first_date = min(ff_to_date[ff_to_date[primary_camera_name].notnull()].index)\n",
    "        \n",
    "        # Print message\n",
    "        print(\"Latest data loaded:\")\n",
    "        print(\"> From: {}\".format(first_date.strftime(\"%d %B %Y\")))\n",
    "        print(\"> To: {} \\n\".format(latest_date.strftime(\"%d %B %Y\")))\n",
    "        print(\"Existing data from:\")\n",
    "        retailers = ff_to_date.columns\n",
    "        for camera in retailers:\n",
    "            print(\"> {}\".format(camera))\n",
    "        print()\n",
    "        \n",
    "        # Backup the old data, if filepath supplied as argument.\n",
    "        if backup_data_filepath != None:\n",
    "            ff_to_date.to_csv(backup_data_filepath, index = True)\n",
    "            print(\"Backing up old data:\")\n",
    "            print(\"> From: {}\".format(first_date.strftime(\"%d %B %Y\")))\n",
    "            print(\"> To: {} \\n\".format(latest_date.strftime(\"%d %B %Y\")))\n",
    "            print(\"Success; old data backed up to:\")\n",
    "            print(\"> {}\\n\".format(backup_data_filepath))\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        ff_to_date = pd.DataFrame(index = [\"Date\"])\n",
    "        \n",
    "        # Dates a long time ago, to get all data to date.\n",
    "        startdate = \"19970101\"\n",
    "        latest_date = \"19970101\"\n",
    "\n",
    "    # Yesterday's date as the end date for the API request.\n",
    "    yesterday = datetime.datetime.now() - datetime.timedelta(1)\n",
    "    enddate = yesterday.strftime(\"%Y%m%d\")\n",
    "\n",
    "    url = \"https://performv4.spring-board.info/outputs/footfalloutput.aspx?useremail={}\\\n",
    "    &userpassword={}\\\n",
    "    &startdate={}\\\n",
    "    &enddate={}\\\n",
    "    &changestartdate=19970101&changeenddate=20970101\".format(useremail, userpassword, startdate, enddate)\n",
    "\n",
    "    # If the data is up to date there is no need to query Springboard for fresh data.\n",
    "    if (latest_date + datetime.timedelta(1)) < datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0):\n",
    "\n",
    "        # Make the request to get the full html page\n",
    "        get_request = requests.get(url)\n",
    "        print(\"Collecting data from Springboard:\")\n",
    "        print(\"> From {}\".format((latest_date + datetime.timedelta(1)).strftime(\"%d %B %Y\")))\n",
    "        print(\"> To {} \\n\".format(yesterday.strftime(\"%d %B %Y\")))\n",
    "\n",
    "        # Use BeasutifulSoup to parse the HTML\n",
    "        html_doc = get_request.text\n",
    "        soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "\n",
    "        # Extract the data from the HTML table.\n",
    "        data = []\n",
    "        table = soup.find(\"table\")\n",
    "        rows = table.find_all(\"tr\")\n",
    "        for r in rows:\n",
    "            cols = r.find_all(\"td\")\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            data.append([ele for ele in cols if ele])\n",
    "\n",
    "        # Convert to a pandas dataframe\n",
    "        new_data = pd.DataFrame(data[1:], columns = data[0])\n",
    "\n",
    "        # Check the API request has returned the required columns, raise an error if not.\n",
    "        required_columns = [\"InCount\", \"OutCount\"]\n",
    "        for column in required_columns:\n",
    "            if column not in new_data.columns:\n",
    "                raise SpringboardError(\"Required columns not returned by the API request.\")\n",
    "            else:\n",
    "                # If the columns are present, convert them to numeric.\n",
    "                new_data[column] = pd.to_numeric(new_data[column])\n",
    "\n",
    "        # Convert FootfallDateTime column to proper datetime\n",
    "        # Add in a properly formatted date_time column\n",
    "        new_data[\"FootfallDateTime\"] = [parser.parse(new_data.loc[row,\"FootfallDate\"] + \" \"\\\n",
    "                                                               + new_data.loc[row,\"FootfallTime\"]) \\\n",
    "                                         for row in range(len(new_data))]\n",
    "\n",
    "        # Add in DayTotal columns to sum all footfall for the day per location\n",
    "        new_data[\"InDayTotal\"] = new_data.groupby([\"LocationName\",\"FootfallDate\"])[\"InCount\"].transform(\"sum\")\n",
    "        new_data[\"OutDayTotal\"] = new_data.groupby([\"LocationName\",\"FootfallDate\"])[\"OutCount\"].transform(\"sum\")\n",
    "\n",
    "        # Add InOutMax column\n",
    "        new_data[\"InOutMax\"] = new_data[[\"InDayTotal\",\"OutDayTotal\"]].max(axis=1)\n",
    "\n",
    "        # Group the data by date & location\n",
    "        grouped_df = new_data.groupby([\"FootfallDate\", \"LocationName\"])[\"InOutMax\"]\\\n",
    "        .aggregate(\"first\").unstack().fillna(0).astype(\"int\")\n",
    "\n",
    "        # Reset the index to the Date column\n",
    "        grouped_df.reset_index(level = 0, inplace = True)\n",
    "        grouped_df.rename(columns={\"FootfallDate\":\"Date\"}, inplace = True)\n",
    "        grouped_df[\"Date\"] = pd.to_datetime(grouped_df[\"Date\"], dayfirst = True)\n",
    "        grouped_df.set_index(\"Date\", inplace = True)\n",
    "\n",
    "        # Sort by the new Date index\n",
    "        grouped_df.sort_index(inplace=True)\n",
    "\n",
    "        # Combine the old_data and new_data dataframes.\n",
    "        frames = [ff_to_date, grouped_df]\n",
    "        new_df = pd.concat(frames)\n",
    "\n",
    "        # Save the new data\n",
    "        new_df.to_csv(new_data_filepath, index = True)\n",
    "        print(\"Saving new data:\")\n",
    "        print(\"> From: {}\".format(first_date.strftime(\"%d %B %Y\")))\n",
    "        print(\"> To: {} \\n\".format(yesterday.strftime(\"%d %B %Y\")))\n",
    "        print(\"Success; new data saved to:\")\n",
    "        print(\"> {}\\n\".format(new_data_filepath))\n",
    "\n",
    "    else:\n",
    "        print(\"Data is up to date.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest data loaded:\n",
      "> From: 01 February 2017\n",
      "> To: 09 September 2017 \n",
      "\n",
      "Existing data from:\n",
      "> Darwin & Wallace\n",
      "> General Store\n",
      "> Grosvenor Bridge Link\n",
      "> Mother\n",
      "\n",
      "Backing up old data:\n",
      "> From: 01 February 2017\n",
      "> To: 09 September 2017 \n",
      "\n",
      "Success; old data backed up to:\n",
      "> ../data_store/footfall_actual_daily_data_BACKUP.csv\n",
      "\n",
      "Collecting data from Springboard:\n",
      "> From 10 September 2017\n",
      "> To 17 September 2017 \n",
      "\n",
      "Saving new data:\n",
      "> From: 01 February 2017\n",
      "> To: 17 September 2017 \n",
      "\n",
      "Success; new data saved to:\n",
      "> ../data_store/footfall_actual_daily_data.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_email = \"your@email.com\"\n",
    "my_password = \"yourpassword\"\n",
    "new_filepath = \"../data_store/footfall_actual_daily_data.csv\"\n",
    "backup_filepath = \"../data_store/footfall_actual_daily_data_BACKUP.csv\"\n",
    "camera = \"Grosvenor Bridge Link\"\n",
    "\n",
    "Springboard_data(useremail = my_email,\n",
    "                 userpassword = my_password,\n",
    "                 new_data_filepath = new_filepath,\n",
    "                 backup_data_filepath = backup_filepath,\n",
    "                 primary_camera_name = camera)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
